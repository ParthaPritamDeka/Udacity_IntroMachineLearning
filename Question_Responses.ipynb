{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Question Responses for the \"Identifying Enron Fraud\" Project\n",
      "\n",
      "##A Python Scikit-Learn Machine Learning Project, offered by Udacity.com\n",
      "\n",
      "###By: Tyler Byers\n",
      "\n",
      "###Date: 16 February 2015\n",
      "\n",
      "###Contact: tybyers@gmail.com\n",
      "\n",
      "This document contains my responses for the six questions that are part of the final project for Udacity's [Intro Machine Learning course](https://www.udacity.com/course/ud120).  \n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 1: About the Project\n",
      "\n",
      "The goal of this project was to use machine learning to identify persons of interest (POIs) in the Enron corporate fraud case.  We were given a dataset with 146 data points (i.e. \"people\"), each of which has 21 features.  The features were financial features (such as salary, bonus, and stock options) and email features (such as number of messages sent, messages sent to POIs, and number of messages received).  Of the 146 people in the data set, there were 18 POIs.  Using machine learning to identify the POIs is useful because of complexity of the data set.  It allows us to try to find patterns to detect POIs.  In this way, we can create a model that then may help us identify POIs from *new* data -- if a new person and their data are sent through the model, the model can then identify whether that new person may be a POI or not.  \n",
      "\n",
      "We used the Python scikit-learn package to create our models.  We first loaded our data into a Python dictionary, called `data_dict`. One of the first steps was to remove outliers from the data.  I removed two data points before continuing to create the model:\n",
      "```python\n",
      "data_dict.pop('TOTAL',0)\n",
      "data_dict.pop('LOCKHART EUGENE E', 0)\n",
      "```\n",
      "I removed the 'TOTAL' line because that's merely a spreadsheet tally line for the other 145 data records.  I also removed Eugene Lockhart because, upon inspection, I noticed that every single one of his features were either \"NaN\" or 0, so his information is completely useless.  I also considered removing Kenneth Lay and Jeffrey Skilling from the data, because their financial features in particular are large outliers when compared to the rest of the data; however, I decided to leave their records in the data because they are an important part of the Enron story -- if you can't identify those two as POIs, then the Enron story completely changes!  I also considered removing \"The Travel Agency in the Park,\" which is a \"person\" in our data; however, I left that line in because in a legal sense, sometimes businesses can be considered as a person.  So if we have a POI that's actually a business, that could be valid information to know as well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 2: Feature Creation and Selection\n",
      "\n",
      "The bulk of my time on this project was spent trying to find and/or create the best features to use for my model.  Since this is a multi-part question, I will address each section specifically.\n",
      "\n",
      "####Q 2.1 Final Features Used\n",
      "I went through several processes to find the final features for my model. When I first started out, I saved my `data_dict` off to a *.csv* file and then opened up the file in RStudio to do some exploratory data analysis (EDA) in R (I initially tried to do the EDA using ggplot in Python, but I was having a lot of trouble handling all the NaN values, and some other difficulties, so I felt I could do my EDA much faster in R).  Those EDA charts can be be found in my [github repo](https://github.com/tybyers/Udacity_IntroMachineLearning/blob/master/enron_exploration.pdf).  After looking at the plots, I threw several features out, because they were unhelpful for one reason or another ('email_address', 'loan_advances', 'restricted_stock_deferred', 'director_fees').  Then, using at the plots I made -- a few scatterplots but mostly histograms -- I tried selecting features by hand, using an AdaBoost classifier, to see what combinations were giving me the best scores.  These scores are recorded in the \"By Hand Feature Selection\" section of [my workflow iPython notebook](http://nbviewer.ipython.org/github/tybyers/Udacity_IntroMachineLearning/blob/master/workflow.ipynb).  These best features I could come up with were: `features_list =  ['poi', 'salary', 'bonus', 'expenses', 'deferral_payments']`.  Of note, the feature selection also included the features I created (mentioned in the next section).\n",
      "\n",
      "I tried two other methods of feature selection.  The methods, and associated \"best\" feature list for each, were:\n",
      "\n",
      " * [Recursive Feature Elimination](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) using an SVC selector.  Best feature combination: `features_list = ['deferral_payments', 'deferred_income', 'total_stock_value', 'total_payments', 'exercised_stock_options']`\n",
      " * [Tree-Based feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection).  Best feature combination: `features_list = ['to_messages', 'deferral_payments', 'expenses', 'deferred_income', 'long_term_incentive']`.\n",
      "\n",
      "However, the prediction performance of the final model using the auto-feature selections was worse than for my hand-picked feature list, so my final choice was to use `features_list =  ['poi', 'salary', 'bonus', 'expenses', 'deferral_payments']`.\n",
      "\n",
      "####Q 2.2 Feature Creation\n",
      "\n",
      "I created four new features using existing features.  As we did in class (Lesson 11), I created `fraction_to_poi` and `fraction_from_poi`  features, to see if perhaps I could identify a POI by whether that person had a certain percentage of his/her outgoing or incoming emails with a POI.  I also created `expenses_per_salary` because I thought I could see from a histogram that people with high expense/salary ratios could be possibly flagged as POIs.  Finally, I created a feature `fraction_emails_with_poi` which is the fraction of *all* a person's emails that involved a POI, rather than just limiting it to incoming/outgoing emails.  However, none of these features made the final cut.  If I throw them into my final model, along with the four hand-selected features, the feature importances are:\n",
      "\n",
      " * expenses: 0.237\n",
      " * **expenses_per_salary: 0.178**\n",
      " * salary: 0.156\n",
      " * **fraction_emails_with_poi: 0.134**\n",
      " * bonus: 0.099\n",
      " * deferral_payments: 0.094\n",
      " * **fraction_to_poi: 0.066**\n",
      " * **fraction_from_poi: 0.036**\n",
      " \n",
      "Interestingly, while two of the created features have higher importance levels than my selected features `bonus` and `deferral_payments`, the performance of the model as a whole is worse with these variables (as judged by cross-validated precision, recall, and f1 score).  Am I just \"fitting to the Kaggle leaderboard?\"  It's hard to say.  This is one of the difficulties with doing machine learning on such a small data set.\n",
      "\n",
      "####Q 2.3 Feature Scaling\n",
      "\n",
      "For the final model, I did not have to do feature scaling, because I used the AdaBoost classifier, which uses boosted decision trees.  Decision trees do not need feature scaling.  I did implement a feature scaling function when I was testing out classifier types though -- in particular I needed it for the KMeans classifier because that classifier doesn't function well when data from different features are scaled fairly differently.\n",
      "\n",
      "####Q 2.4 Features' Effect on Final Model\n",
      "\n",
      "If I add each of my four new features into the final model, the performance metrics are affected as shown in the table below.  Each extra feature is being added to the final *by itself*, not in addition to the other new features.\n",
      "\n",
      "|Metric    | No New Features | expenses_per_salary | fraction_emails_with_poi|fraction_to_poi|fraction_from_poi|\n",
      "|:--------:|:---------------:|:-------------------:|:-----------------------:|---------------|-----------------|\n",
      "|accuracy  | 0.862348178138  | 0.793072424651      | 0.839527680991          | 0.829268292683| 0.845528455285\n",
      "|precision | 0.623737373737  | 0.214814814815      | 0.31746031746           | 0.311688311688| 0.444444444444\n",
      "|recall    | 0.555555555556  | 0.277777777778      | 0.333333333333          | 0.444444444444| 0.333333333333\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 3: Algorithm Choice\n",
      "\n",
      "I tried out three different algorithms: AdaBoost, Random Forest, KMeans.  I used the algorithm defaults and the final feature list `features_list =  ['poi', 'salary', 'bonus', 'expenses', 'deferral_payments']` and the cross-validated performances were:\n",
      "\n",
      "####Random Forest\n",
      "```markdown\n",
      "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0)\n",
      "            \n",
      "  * average accuracy:  0.844804318489\n",
      "  * average precision:  0.166666666667\n",
      "  * average recall:  0.0555555555556\n",
      "```\n",
      "####AdaBoost\n",
      "```markdown\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\n",
      "  * average accuracy:  0.870895186685\n",
      "  * average precision:  0.569444444444\n",
      "  * average recall:  0.5\n",
      "```\n",
      "####KMeans\n",
      "```markdown\n",
      "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=8, n_init=10,\n",
      "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
      "    verbose=0)\n",
      "    \n",
      "  * average accuracy:  0.32816014395\n",
      "  * average precision:  0.818596264417\n",
      "  * average recall:  0.32816014395\n",
      "```\n",
      "Based on this, I selected the AdaBoost algorithm.  If we were only focused on the precision score, the KMeans algorithm might be a better choice, but taking all four performance parameters into consideration leaves AdaBoost as the obvious choice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 4 -- Parameter Tuning\n",
      "\n",
      "After choosing an algorithm (or as part of the algorithm-choosing process), we can try to optimize the prediction capabilities of the algorithm by tuning some of the parameters. For AdaBoost, the tunable parameters are the number of estimators (`n_estimators`), learning rate(`learning_rate`), and `algorithm` (`SAMME.R` or `SAMME`).  If we don't tune the parameters, we might not get the optimum prediction that we might be able to out of our algorithm.  I tuned the above three parameters; the details of the tuning for each parameter are shown below.\n",
      "\n",
      "####Tuning n_estimators\n",
      "To find the optimal number of estimators, I tested my AdaBoost algorithm with 14 different `n_estimators` values:\n",
      "```python\n",
      "num_iters = [5,10,25,50,100,150,200,250,350,500,750,1000,2000,5000]\n",
      "```\n",
      "For each iteration, I ran the algorithm (with `algorithm = SAMME.R`, `learning_rate = 1`, and `random_state = 202`), and tested it using the StratifiedKFold Cross validation with 3 folds as provided in the tester.py script.  I stored the four performance values into a pandas dataframe, and plotted the results.  A code snipped and the plot are shown below (note that I kept getting the following error: `facet_wrap scales not yet implemented!`, and the axes didn't show up for the top left chart, so perhaps there is a bug with the ggplot Python package).\n",
      "```python\n",
      "    df = pd.DataFrame({'Num Estimators': n_iter,\n",
      "                       'Accuracy': accuracies,\n",
      "                       'Precision': precisions,\n",
      "                       'Recall': recalls,\n",
      "                       'F1': f1s\n",
      "                       })\n",
      "    dfm = pd.melt(df, id_vars = 'Num Estimators')\n",
      "    p = ggplot(dfm, aes('Num Estimators', 'value')) + geom_line() + facet_wrap('variable')\n",
      "```\n",
      "\n",
      "![n_estimators ggplot](https://lh4.googleusercontent.com/vqN-NGd0uUSG85Z9gE1hlfjsVcTZGxxCUAcGePbmFfU2=w518-h344-p-no)\n",
      "\n",
      "Based on this, I selected n_estimators = 1000.\n",
      "\n",
      "####Tuning learning_rate\n",
      "\n",
      "After selecting n_estimators = 1000, I selected the learning_rate in a similar fashion, using the following learning rates:\n",
      "```python\n",
      "learn_rate = [0.01, 0.03, 0.1, 0.2, 0.3, 0.4, 0.6, 0.75, 0.9, 1.0, 1.1, 1.25, 1.5, 2.0, 2.5, 3.0]\n",
      "```\n",
      "I held the other parameters constant (`algorithm = SAMME.R`, `n_estimators = 1000`, and `random_state = 202`), and had the following performance plots:\n",
      "\n",
      "![learning rate ggplot](https://lh5.googleusercontent.com/4diJ8WMRx101-MovUhc5WYxSIpyrKQfTwCITZG6DxunH=w516-h344-p-no)\n",
      "\n",
      "Based on this, I chose learning_rate = 1 for my final model, which happens to be the default AdaBoost learning rate.\n",
      "\n",
      "####SAMME.R vs SAMME algorithm\n",
      "\n",
      "Finally, I tuned the `algorithm` parameter, holding `n_estimators = 1000`, `learning_rate = 1.0` and `random_state = 202` constant.  Based on the results shown below, I chose SAMME.R for the final model.\n",
      "\n",
      "**SAMME.R**\n",
      "```markdown                            \n",
      " * average accuracy:  0.862348178138\n",
      " * average precision:  0.623737373737\n",
      " * average recall:  0.555555555556\n",
      "```\n",
      "**SAMME**\n",
      "```markdown\n",
      " * average accuracy:  0.827710301395\n",
      " * average precision:  0.429292929293\n",
      " * average recall:  0.5\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 5 -- About Validation\n",
      "\n",
      "Validation in Machine Learning is the process by which you check the validity of your training model by testing the model on an unseen dataset.  The classic way of doing this is to separate your data into a training set and a testing set, say 70% and 30% of your data, respectively, and train your model on the training set.  Then you validate your model by testing it on the testing set, and seeing how your predicted values for your testing set match up to your actual values for your testing set.  A classic error with validation would be to train your model on *all* your data, and then once again use all that data, or a subset of it, to make predictions.  This is a classic way to *overfit* your model, and in general it will likely perform poorly on any new, unseen data.\n",
      "\n",
      "For validation of my model, I used the handy [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html) cross-validation method with 3 folds as found in the Udacity-provided tester.py script.  Early on in my modeling, I changed this to a 10-fold cross-validation strategy; however, I found I was getting results that weren't matching up as well as I'd like with the results against the tester.py script.  This is likely because with 10 folds and only 144 data points and 18 POIs, to keep a ratio of about 12.5% POIs, the StratifiedKFold method can only put 1 or 2 POIs per test set, with a test set size around 12 persons.  I think if we had a much larger data set, that 10 folds would be a better validation method, but 3 folds seemed to be appropriate for a data set of this size."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Question 6 -- Validation Metrics\n",
      "\n",
      "Using the 3-fold StratifiedKFold cross-validation as provided in the tester.py script, my final model had the following average performance metrics:\n",
      "\n",
      "####Accuracy: 0.862348178138\n",
      "This means my model was 86.2% accurate in predicting whether a person was a POI or not.  That is, my model averaged 86.2% accuracy in predicting a 0 or 1 correctly.  It is important to get the accuracy as high as possible; however, it is interesting to note that since only 12.5% of the people in the final data set were POIs (18/144), if my model had predicted *all* 0s, that is, all non-POIs, the accuracy would have been 87.5%.  So, accuracy is important, but not the most important metric in quantifying how good the model is.\n",
      "\n",
      "####Precision:  0.623737373737\n",
      "This means that 62.4% of the people my model classified as POIs were *actually* POIs.  So, the ratio of true_POIs/(false_POIs + true_POIs) was 0.624.  This is important because we don't want to \"falsely accuse\" too many people of being POIs -- so the higher our precision, the lower our percentage of false accusations compared to accurate accusations.\n",
      "\n",
      "####Recall:  0.555555555556\n",
      "This means that 55.6% of the POIs in the data were correctly identified by the model.  In mathematical terms, this is the ratio of true_POIs/(false_non_POIs + true_POIs).  This is important because we want to catch as many of the POIs as possible, to make sure they face justice.  My model should identify about 5 in 9 of the POIs."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}